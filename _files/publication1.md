---
permalink: /publications/
title: Publications
layout: single
---
**Underline indicates student I mentored. * indicates equal contribution.**

### Network Science Insights towards Graph Foundation Models
<ul>
  <li>
      <p>
          <strong>Position: Graph Foundation Models Are Already Here</strong><br>
          <strong>Haitao Mao</strong>*, <u>Zhikai Chen</u>*, <u>Wenzhuo Tang</u>, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, Jiliang Tang <br>
          <strong>ICML 2024 <font color="red">Spotlight (335/9473) </font>  </strong><br> 
          <font color="green">Collaboration with SnapChat and Intel</font> <br>
          [<a href="https://arxiv.org/abs/2402.02216">pdf</a>]
          [<a href="https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58">Primary Blog</a>]
          [<a href="https://medium.com/@jeongiitae/graph-foundation-models-8cca5d31ddb9">Blog</a>]  
          [<a href="https://github.com/CurryTang/Towards-Graph-Foundation-Models-New-perspective-">Reading List 1</a>]
          [<a href="https://github.com/CurryTang/Towards-graph-foundation-models">Reading List 2</a>]
      </p>
    </li>
    <li>
      <p>
          <strong>Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?</strong><br>
          <strong>Haitao Mao</strong>, <u>Zhikai Chen</u>, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang <br>
          <strong>NeurIPS 2023</strong> <br>
          <font color="green">Collaboration with SnapChat</font> <br>
          [<a href="https://arxiv.org/abs/2306.01323.pdf">pdf</a>]
          [<a href="https://github.com/HaitaoMao/Demystify-structural-disparity">Code</a>] 
          [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/NodeClassification.pdf">Slides</a>] 
          [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/Demestify-poster.pdf">Poster</a>] 
          [<a href="https://www.bilibili.com/video/BV1jj411s7h5/?spm_id_from=333.999.0.0&vd_source=85bb42770c1036d2fc85b057595f1054">Video</a>]
      </p>
    </li>
    <li>
        <p>
            <strong>Revisiting Link Prediction: A Data Perspective</strong><br>
            <strong>Haitao Mao</strong>, <u>Juanhui Li</u>, Harry Shomer, <u>Bingheng Li</u>, Wenqi Fan, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang <br>
            <strong>ICLR 2024</strong>  <br>
            <font color="green">Collaboration with SnapChat</font> <br>
            [<a href="https://arxiv.org/abs/2310.00793.pdf">pdf</a>]
            [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/LinkPrediction.pdf">Slides</a>] 
            [<a href="https://www.bilibili.com/video/BV1jj411s7h5/?spm_id_from=333.999.0.0&vd_source=85bb42770c1036d2fc85b057595f1054">Video</a>]
        </p>
    </li>
    <li>
        <p>
            <strong>Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</strong><br>
            <u>Wenzhuo Tang*</u>, <strong>Haitao Mao*</strong>, Danial Dervovic, Ivan Brugere, Saumitra Mishra, Yuying Xie, Jiliang Tang<br>
            <font color="green">Collaboration with JP Morgan</font> <br>
            preprint [<a href="https://arxiv.org/abs/2406.01899">pdf</a>]
        </p>
    </li>
    <li>
        <p>
            <strong>Do Neural Scaling Laws Exist on Graph Self-Supervised Learning</strong><br>
            <u>Qian Ma</u>, <strong>Haitao Mao</strong>, Jingzhe Liu, Zhehua Zhang, Chunlin Feng, Yu Song, Tianfan Fu, Yao Ma<br>
            Preprint [<a href="https://arxiv.org/abs/2408.11243">pdf</a>][<a href="https://github.com/GraphSSLScaling/GraphSSLScaling">code</a>]
        </p>
    </li>
    <li>
      <p>
          <strong>Neural Scaling Law on Graph</strong><br>
          <u>Jingzhe Liu</u>, <strong>Haitao Mao</strong>, <u>Zhikai Chen</u>, Tong Zhao, Neil Shah, Jiliang Tang <br>
          <font color="green">Collaboration with SnapChat</font> <br>
          Preprint [<a href="https://arxiv.org/abs/2402.02054.pdf">pdf</a>][<a href="https://github.com/Liu-Jingzhe/graph-scaling-laws">code</a>]
          [<a href="https://medium.com/@jeongiitae/neural-scaling-laws-on-graphs-do-you-believe-is-there-strong-related-between-model-data-size-ebd139778928">Blog</a>]  
      </p>
    </li> 
</ul>

### LLM mechanisms 
<ul>
  <li>
      <p>
        <strong>Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis</strong> <br>
        Guangliang Liu, <strong>Haitao Mao</strong>, Jiliang Tang, Kristen Johns.<br>
        EMNLP 2024 <br> [<a href="https://arxiv.org/abs/2407.15286">pdf</a>]
      </p>
  </li>
  <li>
      <p>
        <strong>On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept </strong> <br>
        <strong>Haitao Mao*</strong>, Guangliang Liu*, Bochuan Cao, Zhiyu Xue, Kristen Johnson, Jiliang Tang, Rongrong Wang<br>
        Preprint [<a href="https://arxiv.org/abs/2406.02378">pdf</a>]
      </p>
  </li>
  <li>
      <p>
        <strong>A Data Generation Perspective to the Mechanism of In-Context Learning </strong> <br>
        <strong>Haitao Mao</strong>, Guangliang Liu, Yao Ma, Rongrong Wang, Jiliang Tang <br>
        Preprint [<a href="https://arxiv.org/abs/2402.02212">pdf</a>]
      </p>
  </li>
</ul>

### Graph Foundation Models
<ul>
  <li>
        <p>
            <strong>Text-space Graph Foundation Models: a Comprehensive Benchmark and New Insights</strong><br>
            <u>Zhikai Chen</u>, <strong>Haitao Mao</strong>, Jingzhe Liu, Yu Song, Bingheng Li, Wei Jin, Bahare Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, Jiliang Tang <br>
            <font color="green">collaboration with Google</font> <br>
            <strong>NeurIPS 2024 Dataset & Benchmark Track</strong><br>
            [<a href="https://arxiv.org/abs/2406.10727#:~:text=Text%2Dspace%20Graph%20Foundation%20Models%3A%20Comprehensive%20Benchmarks%20and%20New%20Insights,-Zhikai%20Chen%2C%20Haitao&text=Given%20the%20ubiquity%20of%20graph,has%20recently%20garnered%20significant%20interests.">pdf</a>][<a href="https://github.com/CurryTang/TSGFM">Code</a>]
        </p>
    </li>
  <li>
      <p>
          <strong>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs </strong><br>
          <u>Zhikai Chen</u>, <strong>Haitao Mao</strong>, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang <br>
          <strong>SIGKDD Explorations 2023</strong> <br>
          <font color="green">Collaboration with Baidu</font> <br>
          [<a href="https://arxiv.org/abs/2307.03393.pdf">pdf</a>]
          [<a href="https://github.com/CurryTang/Graph-LLM">Code</a>]
          [<a href="https://www.cse.msu.edu/~tangjili/talks/LLMs-LOG.pdf">Slides</a>]
        </p>
    </li>
  <li>
        <p>
            <strong>Label-free Node Classification on Graphs with Large Language Models (LLMS)</strong><br>
            <u>Zhikai Chen</u>, <strong>Haitao Mao</strong>, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang <br>
            <strong>ICLR 2024</strong> <br> 
            <font color="green">Collaboration with Amazon</font> <br>
            [<a href="https://arxiv.org/abs/2310.04668.pdf">pdf</a>]
            [<a href="https://github.com/CurryTang/LLMGNN">Code</a>]
            [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/LABELFREE.pdf">Slides</a>]
        </p>
    </li>
    <li>
        <p>
            <strong>A Pure Transformer Pretraining Framework on Text-attributed Graphs</strong><br>
            <u>Yu Song</u>, <strong>Haitao Mao</strong>, Jiachen Xiao, Jingzhe Liu, Zhikai Chen, Wei Jin, Carl Yang, Jiliang Tang, Hui Liu<br>
            Preprint [<a href="https://arxiv.org/abs/2406.13873v1">pdf</a>]
        </p>
    </li>
    <li>
        <p>
            <strong>Universal Link Predictor by In-Context Learning on Graphs</strong><br>
            <u>Kaiwen Dong</u>, <strong>Haitao Mao</strong>, Zhichun Guo, Nitesh Chewla<br>
            Preprint [<a href="https://arxiv.org/abs/2402.07738.pdf">pdf</a>]
        </p>
    </li>
  <li>
      <p>
          <strong>Graph Machine Learning in the Era of Large Language Models (LLMs)</strong><br>
          Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang,<br> <strong>Haitao Mao</strong>, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li<br>
          [<a href="https://arxiv.org/abs/2404.14928">pdf</a>]
      </p>
    </li>
      <li>
      <p>
          <strong>LPFormer: An Adaptive Graph Transformer for Link Prediction</strong><br>
          Harry Shomer, Yao Ma, <strong>Haitao Mao</strong>,  Juanhui Li, Bo Wu, Jiliang Tang <br>
          <strong>KDD 2024</strong> <br>  
          [<a href="https://arxiv.org/abs/2310.11009.pdf">pdf</a>]
          [<a href="https://github.com/HarryShomer/LPFormer">Code</a>]
      </p>
    </li>
</ul>

### New Graph Applications 

**Graph Structure in the User Behavior**
<ul>
  <li>
      <p>
          <strong>Whole Page Unbiased Learning to Rank </strong><br>
          <strong>Haitao Mao</strong>, Lixin Zou, Yujia Zheng, Jiliang Tang, Xiaokai Chu, Jiashu Zhao, Dawei Yin <br> 
          <strong> WebConference 2024</strong> <font color="red"><strong>Oral </strong> <strong>(198/2,008)</strong> </font> [<a href="https://arxiv.org/abs/2210.10718.pdf">pdf</a>]<br>
          <font color="green">Work During Internship in Baidu</font> <br>
      </p>
    </li>
</ul>


**Privacy-preserve Graph Structure Learning**
<ul>
  <li>
    <p>
        <strong>Source Free Graph Unsupervised Domain Adaptation </strong><br>
        <strong>Haitao Mao</strong>, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, Dongmei Zhang <br>
        <strong><font color="red">WSDM 2024 Best Paper Honor Mention (3/615)</font></strong> <br>
        <font color="green">Work During Internship in Microsoft Research Asia</font> <br>
        [<a href="https://arxiv.org/abs/2112.00955.pdf">pdf</a>]
        [<a href="https://haitaomao.github.io/categories/sourcefree/">Blog</a>]
        [<a href="https://github.com/HaitaoMao/SOGA">Code</a>]
    </p>  
  </li>
</ul>

**Graph Structure in Leaning to Optimize** 
<ul>
    <li>
      <p>
          <strong>PDHG-Unrolled Learning-to-Optimize Method for Large-Scale Linear Programming</strong><br>
          Bingheng Li, Linxin Yang, Yupeng Chen, Senmiao Wang, Qian Chen, <strong>Haitao Mao</strong>, Yao Ma, Akang Wang, Tian Ding, Jiliang Tang, Ruoyu Sun <br>
          <strong>ICML 2024</strong><br> 
          [<a href="https://arxiv.org/abs/2406.01908">pdf</a>]
      </p>
    </li>
</ul>


**Graph Structure in the Finance Domain** 
<ul>
    <li>
      <p>
        <strong> Company Competition Graph </strong> <br>
        Yanci Zhang, Yutong Lu, <strong>Haitao Mao</strong>, Jiawei Huang, Cien Zhang, Xinyi Li, Rui Dai <br>
        <strong>MAF 2024</strong> <br>
        <font color="green">Collaboration with Wharton Data Center</font> <br>
        [<a href="https://arxiv.org/abs/2304.00323.pdf">pdf</a>]<br>
      </p>
    </li>
  <li>
      <p>
        <strong> Form 10-K Itemization </strong> <br>
        Yanci Zhang, Mengjia Xia, Mingyang Li, <strong>Haitao Mao</strong>, Yutong Lu, Yupeng Lan, Jinlin Ye, Rui Dai <br>
        <font color="green">Collaboration with Wharton Data Center</font> <br>
        Preprint [<a href="https://arxiv.org/abs/2303.04688.pdf">pdf</a>]
      </p>
    </li>
</ul>

**Benchmarking**
<ul>
  <li>
      <p>
        <strong> Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking </strong> <br>
        <u>Juanhui Li*</u>, Harry Shomer*, <strong>Haitao Mao</strong>, Shenglai Zeng, Yao Ma, Neil Shah, Jiliang Tang, Dawei Yin <br>
        <strong>NeurIPS 2023 Datasets & Benchmarks Track</strong> <br> 
        <font color="green">Collaboration with SnapChat</font> <br>
        [<a href="https://arxiv.org/pdf/2306.10453.pdf">pdf</a>]
        [<a href="https://github.com/Juanhui28/HeaRT">Code</a>]
        [<a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202023/73552.png?t=1697474274.198515">Poster</a>]
      </p>
    </li>
    <li>
      <p>
          <strong>Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark</strong><br>
          <u>Xiaowei Qian</u>*, Zhimeng Guo*, Jialiang Li, <strong>Haitao Mao</strong>, Bingheng Li, Suhang Wang, Yao Ma  <br>
          <strong>KDD ADS Track 2024</strong> <br>
          [<a href="https://arxiv.org/pdf/2403.06017.pdf">pdf</a>][<a href="https://github.com/XweiQ/Benchmark-GraphFairness">Code</a>]
      </p>
    </li>
</ul>

### Neuron Network Analysis 
<ul>
  <li>
      <p>
        <strong> Neuron Campaign for Initialization Guided by Information Bottleneck Theory </strong><br>
        <strong>Haitao Mao*</strong>, Xu Chen*, Qiang Fu*, Lun Du*, Shi Han, Dongmei Zhang <br>
        <font color="red"><strong>CIKM2021 Best Short Paper (1/626)</strong></font><br>
        <font color="green">Work During Internship in Microsoft Research Asia</font> <br>
        [<a href="https://arxiv.org/pdf/2108.06530.pdf">pdf</a>]
        [<a href="https://github.com/HaitaoMao/Neuron-Campaign-for-Initialization-Guided-by-Information-Bottleneck-Theory">Github</a>]
        [<a href="https://haitaomao.github.io/categories/neuronCampaign/">Blog</a>]
        [<a href="https://zhuanlan.zhihu.com/p/398198523">Chinese Blog</a>]
        [<a href="https://github.com/haitaomao/haitaomao.github.io/blob/master/_files/CIKM2021/Init_poster.pdf">Offical Poster</a>]
        [<a href="https://github.com/haitaomao/haitaomao.github.io/blob/master/_files/CIKM2021/CIKM21_Neuron_Campaign_for_Initialization_Guided_by_Information_Bottleneck_Theory.pdf">Offical Slides</a>]
        [<a href="https://github.com/haitaomao/haitaomao.github.io/blob/master/_files/CIKM2021/Init_video.mp4">Offical Video</a>]
        [<a href="https://mp.weixin.qq.com/s/PEt7m_iadPGm9puO0S0nHw">AI TIME Introduction</a>]
        [<a href="https://github.com/haitaomao/haitaomao.github.io/blob/master/_files/CIKM2021/AITime%20CIKM21%20-%20Neuron%20Campaign.pdf">AI TIME presentation Slides</a>]
        [<a href="https://www.bilibili.com/video/BV1fL411V7FP?spm_id_from=333.1007.top_right_bar_window_history.content.click">AI TIME Presentation Video</a>]
        [<a href="https://mp.weixin.qq.com/s/V0pwLwTR-rVpe8h5NL_u3g">AI TIME Report</a>]
      </p>
    </li>
    <li>
      <p>
        <strong>Neuron with Steady Response Leads to Better Generalization</strong><br>
        <strong>Haitao Mao*</strong>, Lun Du*, Qiang Fu*, Xu Chen*, Wei Fang, Shi Han, Dongmei Zhang <br>
        <strong>NeurIPS2022</strong><br> 
        <font color="green">Work During Internship in Microsoft Research Asia</font> <br>
        [<a href="https://arxiv.org/pdf/2111.15414.pdf">pdf</a>]
        [<a href="https://github.com/HaitaoMao/Neuron-with-Steady-Response-Leads-to-Better-Generalization">Code</a>] 
        [<a href="https://neurips.cc/virtual/2022/poster/54444">Offical Video</a>]
        [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/NSR-NeurIPS-version.pdf">Official Slides</a>]
        [<a href="https://www.bilibili.com/video/BV19d4y1c7Lt/?spm_id_from=333.999.0.0&vd_source=85bb42770c1036d2fc85b057595f1054">Long Presentation Video</a>]
        [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/NSR-AITIME.pdf">Long Presentation Slides</a>]
        [<a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54444.png?t=1668603047.5147302">Poster</a>]
        [<a href="https://mp.weixin.qq.com/s/A45YqMcQeULULGFL05qBCA">Microsoft Research Asia News</a>]
      </p>
    </li>
</ul>


### New LLM Applications in Data Mining  
<ul>
<li>
      <p>
        <strong> A Large Scale Search Dataset for Unbiased Learning to Rank </strong> <br>
        <strong>Haitao Mao*</strong>, Lixin Zou*, Xiaokai Chu, Jiliang Tang, Shuaiqiang Wang, Wenwen ye, Dawei yin. <br>
        <strong>NeurIPS 2022 Datasets & Benchmarks Track</strong> <br> 
        <font color="green">Work During Internship in Baidu</font> <br>
        [<a href="https://openreview.net/pdf?id=EZcHYuU_9E">pdf</a>]
        [<a href="https://github.com/ChuXiaokai/baidu_ultr_dataset">Code1</a>]
        [<a href="https://github.com/ChuXiaokai/WSDMCUP_BaiduPLM_Paddle">Code2</a>]
        [<a href="https://haitaomao.github.io/baidu_ultr_page/">Dataset Homepage1</a>]
        [<a href="https://searchscience.baidu.com/dataset.html">Dataset Homepage2</a>]
        [<a href="https://www.bilibili.com/video/BV1ZP411N75k/?spm_id_from=333.999.0.0">Long Presentation Video</a>]
        [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/Baidu-ULTR-MLNLP.pdf">Long Presentation Slides</a>]
        [<a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55768.png?t=1669701622.351326">Poster</a>]
      </p>
    </li>
    <li>
      <p>
        <strong>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation </strong><br>
        Wei Jin*, <strong>Haitao Mao*</strong>, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Cheng, Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun, Jiliang Tang, Bing Yin, and Xianfeng Tang  <br>
        <strong>NeurIPS 2022 Datasets and Benchmarks Track</strong> <br> 
        <font color="green">Collaboration with Amazon</font> <br>
        [<a href="https://arxiv.org/pdf/2307.09688.pdf">pdf</a>]
        [<a href="https://kddcup23.github.io/">Homepage</a>]
        [<a href="https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge">Instructions</a>]
        [<a href="https://github.com/HaitaoMao/Amazon-M2">Code</a>]
        [<a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202023/73435.png?t=1699921948.3531067">Poster</a>]
      </p>
    </li>
</ul>




#### Others
<ul>
    <li>
    <p>
        <strong>AI4Science101: Graph Machine Learning</strong><br>
        <strong>Haitao Mao</strong>, Yuanqi Du, Yanbang Wang<br> 
        [<a href="https://ai4science101.github.io/blogs/graph_machine_learning/">blog page</a>]
    </p>
  </li>
    <li>
      <p>
          <strong>Alternately Optimized Graph Neural Network </strong><br>
          Haoyu Han, Xiaorui Liu, <strong>Haitao Mao</strong>,  MohamadAli Torkamani, Feng Shi, Victor Lee, Jiliang Tang  <br>
          <font color="green">Collaboration with TigerGraph</font> <br>
          <strong>ICML 2023</strong>  <br>
          [<a href="https://arxiv.org/pdf/2206.03638.pdf">pdf</a>]
          [<a href="https://github.com/haoyuhan1/ALT-OPT/">Code</a>]
      </p>
    </li>
</ul>


