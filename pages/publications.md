---
permalink: /publications/
title: Publications
layout: single
---
**Underline indicates student I mentored. * indicates equal contribution.**

The full paper list can be found at [Google Scholar](https://scholar.google.com/citations?user=3GmlKM4AAAAJ&hl=en)

### Moral Reasoning with LLMs
<ul>
  <li>
      <p>
        <strong>A Survey to Recent Progress Towards Understanding In-Context Learning</strong> <br>
        <strong>Haitao Mao</strong> et al.  <br>
        NAACL 2025 Findings <br>
        [<a href="https://arxiv.org/abs/2402.02212">pdf</a>][<a href="https://github.com/HaitaoMao/Awesome-LLM-Mechanism-Analysis">Reading List</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section32">Short Summary</a>]
      </p>
  </li>
  <li>
      <p>
        <strong>Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis</strong> <br>
        Guangliang Liu, <strong>Haitao Mao</strong> et al.<br>
        EMNLP 2024 <strong><font color="red">(Recommendation for best paper)</font></strong> <br>
        [<a href="https://arxiv.org/abs/2407.15286">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section31">Short Summary</a>]
      </p>
  </li>
  <li>
      <p>
        <strong>On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept </strong> <br>
        <strong>Haitao Mao*</strong>, Guangliang Liu* et al.<br>
        Preprint <br>
        [<a href="https://arxiv.org/abs/2406.02378">pdf</a>][<a href="https://github.com/HaitaoMao/LLM-self-correction">code</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section32">Short Summary</a>]
      </p>
  </li>
</ul>

### Graph Foundation Model inspired by Network Science 
<ul>
  <li>
        <p>
            <strong>Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models</strong><br>
            <u>Wenzhuo Tang*</u>, <strong>Haitao Mao*</strong> et al.<br>
            preprint [<a href="https://arxiv.org/abs/2406.01899">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section132">Short Summary</a>][<a href="https://github.com/WenzhuoTang/UniAug">code</a>]
        </p>
    </li>
  <li>
      <p>
          <strong>Position: Graph Foundation Models Are Already Here</strong><br>
          <strong>Haitao Mao</strong>*, <u>Zhikai Chen</u>* et al. <br>
          <strong>ICML 2024 <font color="red">Spotlight (335/9473) </font>  </strong><br> 
          [<a href="https://arxiv.org/abs/2402.02216.pdf">pdf</a>]
          [<a href="https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58">Blog</a>]
          [<a href="https://github.com/CurryTang/Towards-Graph-Foundation-Models-New-perspective-">Reading List 1</a>]
          [<a href="https://github.com/CurryTang/Towards-graph-foundation-models">Reading List 2</a>]
      </p>
    </li>
    <li>
        <p>
            <strong>Revisiting Link Prediction: A Data Perspective</strong><br>
            <strong>Haitao Mao</strong> et al.<br>
            <strong>ICLR 2024</strong>  <br>
            [<a href="https://arxiv.org/abs/2310.00793.pdf">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section131">Short Summary</a>]
            [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/LinkPrediction.pdf">Slides</a>] 
        </p>
    </li>
    <li>
      <p>
          <strong>Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?</strong><br>
          <strong>Haitao Mao</strong> et al.<br>
          <strong>NeurIPS 2023</strong> <br>
          [<a href="https://arxiv.org/abs/2306.01323.pdf">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section134">Short Summary</a>]
          [<a href="https://github.com/HaitaoMao/Demystify-structural-disparity">Code</a>] 
          [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/NodeClassification.pdf">Slides</a>] 
      </p>
    </li>
    <li>
        <p>
            <strong>A Pure Transformer Pretraining Framework on Text-attributed Graphs</strong><br>
            <u>Yu Song</u>, <strong>Haitao Mao</strong> et al.<br>
            <strong>LoG 2024</strong> <br> [<a href="https://arxiv.org/abs/2406.13873v1">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section135">Short Summary</a>]
        </p>
    </li>
    <li>
        <p>
            <strong>Do Neural Scaling Laws Exist on Graph Self-Supervised Learning</strong><br>
            <u>Qian Ma</u>, <strong>Haitao Mao</strong>  et al.<br>
            <strong>LoG 2024</strong> <br>
            [<a href="https://arxiv.org/abs/2408.11243">pdf</a>][<a href="https://github.com/GraphSSLScaling/GraphSSLScaling">code</a>]
        </p>
    </li>
    <li>
      <p>
          <strong>Neural Scaling Law on Graph</strong><br>
          <u>Jingzhe Liu</u>, <strong>Haitao Mao</strong> et al.<br>
          <strong>LoG 2024</strong> <br>
          [<a href="https://arxiv.org/abs/2402.02054.pdf">pdf</a>][<a href="https://github.com/Liu-Jingzhe/graph-scaling-laws">code</a>]
          [<a href="https://medium.com/@jeongiitae/neural-scaling-laws-on-graphs-do-you-believe-is-there-strong-related-between-model-data-size-ebd139778928">Blog</a>]  
      </p>
    </li>
    <li>
        <p>
            <strong>Text-space Graph Foundation Models: a Comprehensive Benchmark and New Insights</strong><br>
            <u>Zhikai Chen</u>, <strong>Haitao Mao</strong> et al.<br>
            <strong>NeurIPS 2024 Dataset & Benchmark Track</strong><br>
            [<a href="https://arxiv.org/abs/2406.10727">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section12">Short Summary</a>][<a href="https://github.com/CurryTang/TSGFM">Code</a>]
        </p>
    </li>
</ul>



### Internship Works
<ul>
  <li>
      <p>
          <strong>Whole Page Unbiased Learning to Rank </strong><br>
          <strong>Haitao Mao</strong>  et al. <br> 
          <strong> WebConference 2024</strong> <font color="red"><strong>Oral </strong> <strong>(198/2,008)</strong> </font> [<a href="https://arxiv.org/abs/2210.10718.pdf">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section21">Short Summary</a>]<br>
      </p>
    </li>
    <li>
    <p>
        <strong>Source Free Graph Unsupervised Domain Adaptation </strong><br>
        <strong>Haitao Mao</strong>  et al. <br>
        <strong><font color="red">WSDM 2024 Best Paper Honor Mention (3/615)</font></strong> <br>
        [<a href="https://arxiv.org/abs/2112.00955.pdf">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section11">Short Summary</a>]
        [<a href="https://haitaomao.github.io/categories/sourcefree/">Blog</a>]
        [<a href="https://github.com/HaitaoMao/SOGA">Code</a>]
    </p>  
  </li>
  <li>
      <p>
        <strong>Neuron with Steady Response Leads to Better Generalization</strong><br>
        <strong>Haitao Mao*</strong> et al. <br>
        <strong>NeurIPS2022</strong><br> 
        [<a href="https://arxiv.org/abs/2111.15414.pdf">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section24">Short Summary</a>]
        [<a href="https://github.com/HaitaoMao/Neuron-with-Steady-Response-Leads-to-Better-Generalization">Code</a>] 
        [<a href="https://github.com/HaitaoMao/HaitaoMao.github.io/blob/master/_files/NSR-AITIME.pdf">Slides</a>]
        [<a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/54444.png?t=1668603047.5147302">Poster</a>]
      </p>
    </li>
    <li>
      <p>
        <strong>Neuron Campaign for Initialization Guided by Information Bottleneck Theory </strong><br>
        <strong>Haitao Mao*</strong>, Xu Chen*, Qiang Fu*, Lun Du*  et al. <br>
        <font color="red"><strong>CIKM2021 Best Short Paper (1/626)</strong></font><br>
        [<a href="https://arxiv.org/abs/2108.06530.pdf">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section24">Short Summary</a>]
        [<a href="https://github.com/HaitaoMao/Neuron-Campaign-for-Initialization-Guided-by-Information-Bottleneck-Theory">Code</a>]
        [<a href="https://haitaomao.github.io/categories/neuronCampaign/">Blog</a>]
        [<a href="https://github.com/haitaomao/haitaomao.github.io/blob/master/_files/CIKM2021/AITime%20CIKM21%20-%20Neuron%20Campaign.pdf">Slides</a>]
      </p>
    </li>
  <li>
      <p>
        <strong>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation </strong><br>
        Wei Jin*, <strong>Haitao Mao*</strong> et al.  <br>
        <strong>NeurIPS 2022 Datasets and Benchmarks Track</strong> <br> 
        [<a href="https://arxiv.org/abs/2307.09688.pdf">pdf</a>]
        [<a href="https://haitaomao.github.io/categories/ResearchSummary/#section221">Short Summary</a>]
        [<a href="https://kddcup23.github.io/">Homepage</a>]
        [<a href="https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge">Instructions</a>]
        [<a href="https://github.com/HaitaoMao/Amazon-M2">Code</a>]
      </p>
    </li>
    <li>
      <p>
        <strong>A Large Scale Search Dataset for Unbiased Learning to Rank </strong> <br>
        <strong>Haitao Mao*</strong>, Lixin Zou* et al. <br>
        <strong>NeurIPS 2022 Datasets & Benchmarks Track</strong> <br> 
        [<a href="https://arxiv.org/abs/2207.03051">pdf</a>][<a href="https://haitaomao.github.io/categories/ResearchSummary/#section21">Short Summary</a>]
        [<a href="https://github.com/ChuXiaokai/baidu_ultr_dataset">Code1</a>]
        [<a href="https://github.com/ChuXiaokai/WSDMCUP_BaiduPLM_Paddle">Code2</a>]
        [<a href="https://searchscience.baidu.com/dataset.html">Dataset Homepage2</a>]
      </p>
    </li>
</ul>


